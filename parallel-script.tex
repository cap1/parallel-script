\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{multirow}
\usepackage{color}
\usepackage{listings}
\usepackage{float}

\author{Christian Mueller}
\title{Parallel Computing Script}

\begin{document}

%Settings for Listings Environment
\newfloat{code}{h}{Listing}
\floatname{code}{Listing}
\lstset{language=Perl,frame=L}

\maketitle
\tableofcontents

\section{Introduction} % (fold)
\label{sec:introduction}
	The computing power has been growing ever since the first integrated circuit,
	but it has an ultimate limit: the speed of light.
	The performance of microprocessors has been increasing by 50\% each year from 1986 till 2002.
	Since then the rate has decreased to only about 20\% per year.
	This gain in performance resulted in the reduction of the size of integrated circuits.
	But as ICs become more dense for faster processing,
	they also consume more power and therefore generate more unwanted heat.
	To overcome the need for higher clock-rates,
	whom are needed to increase the performance on single core processing units,
	one is using multiple cores instead.
	With this parallelism, computers can gain in performance.
	
 	\begin{description}
 		\item[Multicomputer] \hfill \\
 			\begin{itemize}
 				\item distributed-memory
 				\item disjoint address space
 				\item communication via message-passing
 				\item \textsl{i.e. a cluster of networked computers}
 			\end{itemize}
 		\item[Multiprocessor] \hfill \\
 			multiple computing cores inside one machine\\
 			\begin{itemize}
 				\item shared-memory
 				\item single address space
 				\item \textsl{i.e. multiprocessor or multicore machines}
 			\end{itemize}
 		\item[Hybrid] \hfill \\
 			a cluster of multiprocessors
	\end{description}

	Parallel Computing and Distributed Computing may sound similar 
	but differ in the fact that parallel computing only tries to solve one problem,
	where distributed computing is interested in solving a set of problems.
	Although they share some problems likes synchronization,
	communication, consistency and problem decomposition.
	Usually also software needs to fitted for parallel approaches,
	as algorithms can be solved in different ways
	and running of multiple instances of serial code is not very useful in most cases.
% section introduction (end)

\section{Caching} % (fold)
\label{sec:caching}
The classic von Neumann architecture does not contain any cache.
Data and instruction stream are just loaded.
CPUs are connected to the main memory via a bus.
As this is a shared media, and the distance to it is,
in terms of access times, far.
Accessing a chunk of memory takes about 100 CPU cycles.
Cache is located right on the same chip of the CPU and therefore can be access a lot faster,
typically it takes up to 20 cpu cycles to get data from the cache.

\subsection{Caching principles} % (fold)
\label{sub:caching_principles}
	Programs usually enforce some memory access locality,
	which means that the following memory access takes place in a nearby location. 
	This can refer to spatial locality,
	which means that memory locations close in space will be accessed.
	It can also mean temporal locality, which refers to nearby in its temporal meaning.
	Both of these types are usually taken into account when data is cached.
	Depending on these information data is stored in the different levels of cache.
	Level 1 Cache refers then to the cache closest to the CPU 
	nd with growing numbers the cache is farrer away from the CPU.
	With more distance to the CPU the cache gets larger but is slower to access.
% subsection caching_principles (end)

\subsection{Issues} % (fold)
\label{sub:issues}
	Caches add some complexity to loading data.
	The CPU must be able to determine whether the data is already in the cache
	or needs to be loaded from the main memory.
	This overhead must not be bigger than loading the data from the main memory,
	otherwise the cache is useless.
	The mayor issue with cache although is consistency.
	Especially in cases with more than one CPU.
	When data is altered in the Cache one can either directly update the values to the RAM.
	This technique is called ‘write-through’.
	As the CPU has to wait until the data is deposited in the main memory,
	it is usually avoided as it is slower.
	If the CPU can mark data in the cache as ‘dirty’.
	When a new line of cache is loaded to this cache position,
	the dirty line will be written back to the memory.
	This technique is usually used and is referred to as ‘write-back’.
% subsection issues (end)

\subsection{Cache mapping} % (fold)
\label{sub:cache_mapping}
	The cache can be filled in different ways.
	This depends on the architecture of the cache
	and the logic and the principles that fill it.
	Full associative mapping means,
	that a new line can be placed anywhere in the cache.
	This is the most flexible way to add data to the cache
	but also the most difficult one to check if the data is currently in the cache.

	Direct mapped cache mapping defines unique locations in the cache that a line can be stored in. This is the easiest method to lookup if the data has been cached already,
	but as a cache cannot be as large as the main memory,
	chunks of it share a location in the cache.
	This means that certain parts of the memory cannot be cached at the same time,
	which is a drawback.

	N-way set associative stands for a cache mapping,
	where a line can be placed in n different locations in the cache.
	This adds the necessity to determine which lines shall be kept
	and which shall be replaced or evicted.

	Knowledge about the cache mapping can be very helpful to speed up programs.
	If the a matrix is stored in ‘row-major’
	and one loads it ‘column-major’-wise,
	this leads to cachelines being loaded and discarded very often.
% subsection cache_mapping (end)
% section caching (end)

\section{Instruction Level Parallelism} % (fold)
\label{sec:instruction_level_parallelism}
To improve processor performance by having multiple processor components of functional units simultaneously executing program instruction is the main idea of instruction level parallelism (ILP). There are two flavours, Pipelining, where functional units are arranged in stages and Multiple issue, where multiple instructions are initiated simultaneously.

\subsection{Pipelining} % (fold)
\label{sub:pipelining}
	The floating point adder is divided into 7 separate pieces of hardware or functional units.
	The first one fetches the operands, 
	he second compares exponents,
	the third shifts one operand if necessary,
	the fourth unit performs the actual add and so on.
	The last, the seventh unit the stores the result.
	With this type of parallelism every unit can be utilized better
	as none of them is ever involved in the same addition.
	Depending on their position in the Pipeline and the point of view,
	they are several additions ahead or behind.
	If one assumes that each operation takes one nanosecond,
	one floating point addition will take 7 nanoseconds.
	However using pipelining 1000 floating point additions will only take 1006 nanoseconds
	and not 7000 ns.
% subsection pipelining (end)

\subsection{Multiple Issue} % (fold)
\label{sub:multiple_issue}
	This type of processor replicates functional units
	and tries to simultaneously execute different instructions in a program.
	A typical example is to sum up values from two different arrays (x, y)
	and store them in a third (z).
	With a running variable (i),
	the follow additions can be done simultaneously by two different sets of functional units:\\
	\begin{center}
		z[i] = x[i] + y[i];
		\begin{enumerate}
			\item z[1] = x[1] + y[1]
			\item z[2] = x[2] + y[2]
		\end{enumerate}
	\end{center}

	There are two types of Multiple issue, static and dynamic.
	In Static Multiple Issue, the functional units are assigned
	and scheduled at the compile time of the code.
	In Dynamic Multiple Issue, the scheduling takes place at run-time.
	A processor supporting dynamic multiple issue is often called a ‘superscalar’.
% subsection multiple_issue (end)

\subsection{Speculation} % (fold)
\label{sub:speculation}
	In Speculative Execution the computers guesses which parts of the code can be executed in parallel.
	Not in all cases it is just as obvious as above,
	but in some cases it can be done.
	This depends on the code given to the computer.
% subsection speculation (end)

\subsection{Hardware Multithreading} % (fold)
\label{sub:hardware_multithreading}
	Threads offer a great opportunity to do introduce parallelism,
	as the programmer defines clearly what can be done in parallel.
	Supporting multiple threads on hardware level allows the system do useful work
	when the task being currently executed has stalled for any reason.
	Typical reasons are the wait for I/O or data to be loaded from memory.
	Multi-threading is characterized by the typical execution time for a thread.

	Fine-grained multithreading will switch threads in the processor after every instruction,
	thereby skipping threads that are currently stalled.
	This technique avoids wasting machine time,
	as it will skip every potential threat.
	A thread that consists of a long sequence of instruction will have to wait longer,
	as the instructions are only executed one by one.
	Simultaneous multithreading (SMT) is an even finer grained variant,
	that allows multiple threads to make use of multiple functional units.

	Coarse-grained multithreading only switches the threads if the current one stalls
	and has to wait for a time-consuming operation to complete.
	This leads to lesser thread switches,
	which allows threads with longer sequences of instructions to be executed in a row.
	It also is not that much dependent on fast switch between threads.
	Although the processor might idle for some time with this technique.
% subsection hardware_multithreading (end)
% section instruction_level_parallelism (end)
\section{Classification} % (fold)
\label{sec:classification}

\subsection{Single Instruction Multiple Dataset} % (fold)
\label{sub:single_instruction_multiple_dataset}
	This technique utilizes processor that are only capable of a few instructions,
	but can perform these very fast.
	They are used in parallel, executing the same instruction on the different data streams. 
	This was used extensively for vector processing for example.
	This architecture has some drawbacks,
	as all ALUs have to perform the same instruction or remain idle.
	In classic designs they were even required to operate synchronously
	and even todays ALUs do not have an instruction storage.
	Overall this makes SIMD efficient for large data parallel problems,
	but for nothing else.

	\subsubsection{Vector processors} % (fold)
	\label{ssub:vector_processors}
	In contrast to conventional CPUs who work on individual data elements of scalars
	vector processors operate on arrays or on vectors of data.
	They incorporate special register and are able to store a vector of operands 
	and perform an operation simultaneously on their contents.
	Therefore they are related to SIMD.

	Vector processors make use of interleaved memory organized in “banks”,
	that is accessible independently.
	The data they work on can be distributed over muliple banks,
	therefore allowing access by more functional units.
	This memory is also optimized to loading and storing of successive elements.

	This kind of processors is fast and easy to use. 
	Programmers can rely on compilers that are well optimized
	and can identify code that is vectorizable.
	Also they can provide information about code that is not vectorizable,
	which helps re evaluating it.
	Vector processors have a high memory bandwidth by design.
	On the other hand they are specialized on vectors,
	so they are not capable to process irregular data structures very well.
	Also they do not scale,
	as there is a finite limit to their abilities to handle larger problems.
	% subsubsection vector_processors (end)
	
	\subsubsection{Graphic Processing Units} % (fold)
	\label{ssub:graphic_processing_units}
	As these types of specialized processing units became faster,
	they could also be used for other things than graphical computations.
	They incorporate so called shader functions that are part of pipelines.
	These shader functions can be programmed with a few lines of C code.
	They are implicitly parallel, since that is what they were used for in the first place,
	to work on multiple elements of a graphics stream.
	GPUs use the SIMD parallelism although they are not pure SIMD devices.
	% subsubsection graphic_processing_units (end)
% subsection single_instruction_multiple_dataset (end)

\subsection{Multiple Instruction Multiple Dataset} % (fold)
\label{sub:multiple_instruction_multiple_dataset}
As the name suggests,
it describes multiple simultaneous instructions streams operating on multiple data streams.
It typically consists of a collection of fully independent processing units.
Each of these have their own control unit and own ALUs.
The collection of processing units is interconnected,
but communication is implicitly by accessing the same memory and shared data structures.
The multiple processing units are typically organized on a single chip
and can be referred to as multi-core processors.
They are the most common type of shared-memory systems.
Depending on the interconnect to the memory the access times will differ. 
This largely depends on if the memory is really shared or not.
% subsection multiple_instruction_multiple_dataset (end)

\subsection{Shared Memory Systems} % (fold)
\label{sub:shared_memory_systems}
Shared Memory Interconnects use certain types of connections
for all Nodes to communicated with each other. 
Buses can be implemented relatively easy,
but they do not scale well.
As it is a shared medium,
if too many nodes want to access it the overall performance decreases.
They are only suited if the number of nodes to connect is small and scalability is not an issue.
Crossbar interconnections allow some nodes in a network to communicate with each other.
Depending on the connections other nodes are currently using,
not all connections work at the same time.
This enhances performance and makes them faster than buses.
Crossbars also scale better than busses,
but they still suffer from some the amount of wiring and switches needed,
which makes them also more expensive.

The performance of shared memory systems is highly dependent on the speed of the interconnect. As all CPUs use the same memory and do transactions on it using this interconnect.
% subsection shared_memory_systems (end)

\subsection{Distributed Memory Systems} % (fold)
\label{sub:distributed_memory_systems}
In these systems the CPU and the Memory are located more closely together than other CPUs.
They are then connected via an Interconnect.
The most common type is a Cluster.
These systems consist of commodity hardware that is linked via a fast interconnect.
Individual nodes in a cluster perform computations.
These systems are also known as hybrid systems.
They type of interconnect is very important,
as it defines the speed of communication between the nodes.
A typical connection is fiber via Infiniband. 

The Interconnects are separated in two groups, Shared Memory Interconnects and Distributed Memory Interconnects.
Those who allow indirect connections and
those who only allow direct connections between a node and a switch or between switches. 
A toroidal network is a typical type of network used in distributed memory with direct interconnects. 
Others are Hypercubes and Omega networks.
Fully connected networks are not considered as they may be very fast but do not scale at all.

\subsubsection{Interconnects} % (fold)
\label{ssub:interconnects}
	\begin{description}
			\item[Mesh] \hfill \\
			In a two-dimensional Mesh, a node is connected to its neighbors
			and it is usually organized in squares,
			where one node is at least connected to two nodes.
			Theses are the nodes in the corners,
			nodes on the side are connected to three other nodes
			and the ones on the center are connected to four other nodes.
		\item[Crossbar] \hfill \\
			In crossbars, nodes are attached to one side of a two dimensional mesh.
			This enables a scalable and easy to implement network.
		\item[Torus] \hfill \\
			A Tous is like a Mesh but all nodes are connected with four other nodes.
			This is accomplished by connecting the outer nodes on one side,
			with the other outer node on the outer side.
		\item[Hypercube] \hfill \\
			Hypercubes consist at least three-dimensional cubes,
			where the corners are the compute nodes.
			A four-dimensional hypercube can be pictured as having a smaller cube inside a larger one.
		\item[Omega Network] \hfill \\
			This type of Network is currently used.
			It consists of layers of switches and computes.
			These can be added and allow a scalable network,
			that can be extend in both two dimensional directions.
			Butterfly Networks are essential the same
			and Omega Networks can be transformed into them.
			Butterfly networks can be drawn easier but cannot be easily implemented in that way.
	\end{description}
% subsubsection interconnects (end)

\subsubsection{Properties of Networks} % (fold)
\label{ssub:properties_of_networks}
	Networks can be compared using certain properties
	which describe how well their nodes are interconnect with each other 
	and how the networks behave when more nodes shall be added.
	These properties are important if one wants to choose a network for a certain use case.
	The Bisection width defines the number of possible simultaneous communications or connectivity.
	It describes how many connections can take place,
	across an equal divide of the network, between the halves.
	Related is the bisection bandwidth,
	which describes how much data can be transferred simultaneously between halves.
	It sums the bandwidth of the links in the bisection width.

	The latency is defined as the time that elapses between the source starting a transmission
	and the sink receiving the first byte.
	
	The bandwidth is defined as the rate at which the sink receives data after receiving the first bytes.

	The message transmission time is defined as:\\
		$$ T = L + n/b$$
	Where L is latency in seconds,
	n is the length of the message in bytes
	and b is the bandwidth in bytes per second.
% subsubsection properties_of_networks (end)
% subsection distributed_memory_systems (end)
% section classification (end)

\section{Parallel Software} % (fold)
\label{sec:parallel_software}
Software needs to be designed with parallelization in mind,
as software written for serial execution will not result in any speedup on parallel machines.
There is no easy way transforming serial code into parallel code.
Serial programs can be executed in parallel,
but this does never fully leverage the capabilities of a parallel hardware.
This approach gets unfeasible as soon as the complexity of the serial programs becomes an issue.

\subsection{Data- \& Task Parallelism} % (fold)
\label{sub:data_task_parallelism}
	Parallelism can be introduced in two different fashions,
	but both base on a classical divide-and-conquer approach.
	They can also be related to the classification like SIMD, MIMD and MISD.\\
	\textsl{Task parallelism} partitions a larger task into subtasks,
	that are carried out by the cores or computes.

	\textsl{Data parallelism} splits the data to work on into smaller chunks
	and the same task gets carried out on different data on the cores or computes.

	A real world example to distinguish both is a correcting an 300 exams by 3 teaching assistants.
	Data parallelism is used,
	when the professor assigns each TA 100 exams to correct.
	Task parallelism is performed,
	when each of the TA is assigned a certain range of questions in the exams to correct.
	For examples in Perl-code see Listing \ref{code:dataParalellism} and \ref{code:taskParalellism}.

	\begin{code}
	\begin{lstlisting}
	$sum = 0;
	for ($i = 0; $i < $n; $i++) {
		$x = &computeNextValue($foo);
		$sum += $x;
	}
	\end{lstlisting}
	\caption{Data Parallelism}
	\end{code}
	\label{code:dataParalellism}

	\begin{code}
	\begin{lstlisting}
	if ($master) {
		$sum = $my_x;
		foreach $core (@cores) {
			$sum += &receiveValue($core);
		}
	}
	else {
		$x = &computeNextValue($foo);
		&sendValue($master, $x);
	}
	\end{lstlisting}
	\caption{Task parallelism}
	\end{code}
	\label{code:taskParalellism}
% subsection data_task_parallelism (end)

\subsection{Processes \& Threads} % (fold)
\label{sub:processes_&_threads}
Threads are smallest sequence of computer code that can be managed independently by a scheduler. %\cite{wpEnThred}
A thread is in most cases associated with a process,
who creates it.
In contrast to heavy processes,
threads only incorporate their own stack, program counter and identifier,
making them very lightweight.
As the additional structures offered by process are usually not necessary,
threads are very commonly used in parallel software.
Their lightweightendess allows to create them fast and with little overhead.
Forking a new process takes longer needs more resources.
When threads are terminated they are joined with the main process who created them.

\paragraph*{Memory and threading} % (fold)
\label{par:memory_and_threading}
	Depending on the type of memory involved different approaches are taken.
	In a shared memory program,
	a single process is started who forks threads that carry out the tasks.
	In distributed memory programs,
	multiple processes are started and they carry out the tasks.
	Threads cannot leave a machine,
	therefore it it is not feasible in distributed memory programs.
	Although threads might be involved in solving the tasks along with the process in distribute memory environments.\\

	In a shared memory environment,
	threads can be managed in two different ways.
	\textsl{Dynamic threads} The master thread forks a given number of worker threads
	and forks new ones if another one is finished.
	This uses resources efficiently,
	but the creation of threads and their termination might be time consuming.\\
	\textsl{Static threads} are pooled and started upfront.
	The load is spread amongst them,
	but they are not terminated until the final cleanup.
	This is better performance wise,
	but potential can waste system resources.
% paragraph memory_and_threading (end)

\paragraph*{Single Programm, Multiple Data} % (fold)
\label{par:single_programm_multiple_data}
	This directive uses a single binary file that is spread across the compute nodes.
	In the beginning there is prominent if-else condition,
	which decides about the job the binary will carry out.
	The running code will find out at runtime if he is run as a master
	or as a worker.
	The master usually collects and integrates the work done by the workers.\\
	This scheme can be implemented with threads or with processes
	and is very typical for parallel programming.
% paragraph single_programm_multiple_data (end)

\paragraph*{Non-Determinism} % (fold)
\label{par:non_determinism}
	As the handling and scheduling of the threads is done by the operating system,
	the master thread looses control over the time of exection.
	This may result in problems with determinism,
	that have to be considered.
	One has to make sure to think of possible race conditions
	and critical sections of the code.
	The occurring problems can be addressed by using a mutex or a simple lock,
	to ensure the consistency of the data.
% paragraph non_determinism(end)

% subsection processes_&_threads (end)
% section parallel_software (end)
\end{document}