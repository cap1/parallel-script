\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{multirow}
\usepackage{color}

\author{Christian Mueller}
\title{Parallel Computing Script}

\begin{document}

\maktitle
\tableofcontents

\section{Introduction} % (fold)
\label{sec:introduction}
	The computing power has been growing ever since the first integrated circuit,
	but it has an ultimate limit: the speed of light.
	The performance of microprocessors has been increasing by 50\% each year from 1986 till 2002.
	Since then the rate has decreased to only about 20\% per year.
	This gain in performance resulted in the reduction of the size of integrated circuits.
	But as ICs become more dense for faster processing,
	they also consume more power and therefore generate more unwanted heat.
	To overcome the need for higher clock-rates,
	whom are needed to increase the performance on single core processing units,
	one is using multiple cores instead.
	With this parallelism, computers can gain in performance.
	
 	\begin{description}
 		\item[Multicomputer] \hfill \\
 			\begin{itemize}
 				\item distributed-memory
 				\item disjoint address space
 				\item communication via message-passing
 				\item \textsl{i.e. a cluster of networked computers}
 			\end{itemize}
 		\item[Multiprocessor] \hfill \\
 			multiple computing cores inside one machine\\
 			\begin{itemize}
 				\item shared-memory
 				\item single address space
 				\item \textsl{i.e. multiprocessor or multicore machines}
 			\end{itemize}
 		\item[Hybrid] \hfill \\
 			a cluster of multiprocessors
	\end{description}

	Parallel Computing and Distributed Computing may sound similar 
	but differ in the fact that parallel computing only tries to solve one problem,
	where distributed computing is interested in solving a set of problems.
	Although they share some problems likes synchronization,
	communication, consistency and problem decomposition.
	Usually also software needs to fitted for parallel approaches,
	as algorithms can be solved in different ways
	and running of multiple instances of serial code is not very useful in most cases.
% section introduction (end)

\section{Caching} % (fold)
\label{sec:caching}
The classic von Neumann architecture does not contain any cache.
Data and instruction stream are just loaded.
CPUs are connected to the main memory via a bus.
As this is a shared media, and the distance to it is,
in terms of access times, far.
Accessing a chunk of memory takes about 100 CPU cycles.
Cache is located right on the same chip of the CPU and therefore can be access a lot faster,
typically it takes up to 20 cpu cycles to get data from the cache.

\subsection{Caching principles} % (fold)
\label{sub:caching_principles}
	Programs usually enforce some memory access locality,
	which means that the following memory access takes place in a nearby location. 
	This can refer to spatial locality,
	which means that memory locations close in space will be accessed.
	It can also mean temporal locality, which refers to nearby in its temporal meaning.
	Both of these types are usually taken into account when data is cached.
	Depending on these information data is stored in the different levels of cache.
	Level 1 Cache refers then to the cache closest to the CPU 
	nd with growing numbers the cache is farrer away from the CPU.
	With more distance to the CPU the cache gets larger but is slower to access.
% subsection caching_principles (end)

\subsection{Issues} % (fold)
\label{sub:issues}
	Caches add some complexity to loading data.
	The CPU must be able to determine whether the data is already in the cache
	or needs to be loaded from the main memory.
	This overhead must not be bigger than loading the data from the main memory,
	otherwise the cache is useless.
	The mayor issue with cache although is consistency.
	Especially in cases with more than one CPU.
	When data is altered in the Cache one can either directly update the values to the RAM.
	This technique is called ‘write-through’.
	As the CPU has to wait until the data is deposited in the main memory,
	it is usually avoided as it is slower.
	If the CPU can mark data in the cache as ‘dirty’.
	When a new line of cache is loaded to this cache position,
	the dirty line will be written back to the memory.
	This technique is usually used and is referred to as ‘write-back’.
% subsection issues (end)

\subsection{Cache mapping} % (fold)
\label{sub:cache_mapping}
	The cache can be filled in different ways.
	This depends on the architecture of the cache
	and the logic and the principles that fill it.
	Full associative mapping means,
	that a new line can be placed anywhere in the cache.
	This is the most flexible way to add data to the cache
	but also the most difficult one to check if the data is currently in the cache.

	Direct mapped cache mapping defines unique locations in the cache that a line can be stored in. This is the easiest method to lookup if the data has been cached already,
	but as a cache cannot be as large as the main memory,
	chunks of it share a location in the cache.
	This means that certain parts of the memory cannot be cached at the same time,
	which is a drawback.

	N-way set associative stands for a cache mapping,
	where a line can be placed in n different locations in the cache.
	This adds the necessity to determine which lines shall be kept
	and which shall be replaced or evicted.

	Knowledge about the cache mapping can be very helpful to speed up programs.
	If the a matrix is stored in ‘row-major’
	and one loads it ‘column-major’-wise,
	this leads to cachelines being loaded and discarded very often.
% subsection cache_mapping (end)
% section caching (end)

\section{Instruction Level Parallelism} % (fold)
\label{sec:instruction_level_parallelism}
To improve processor performance by having multiple processor components of functional units simultaneously executing program instruction is the main idea of instruction level parallelism (ILP). There are two flavours, Pipelining, where functional units are arranged in stages and Multiple issue, where multiple instructions are initiated simultaneously.

\subsection{Pipelining} % (fold)
\label{sub:pipelining}
	The floating point adder is divided into 7 separate pieces of hardware or functional units.
	The first one fetches the operands, 
	he second compares exponents,
	the third shifts one operand if necessary,
	the fourth unit performs the actual add and so on.
	The last, the seventh unit the stores the result.
	With this type of parallelism every unit can be utilized better
	as none of them is ever involved in the same addition.
	Depending on their position in the Pipeline and the point of view,
	they are several additions ahead or behind.
	If one assumes that each operation takes one nanosecond,
	one floating point addition will take 7 nanoseconds.
	However using pipelining 1000 floating point additions will only take 1006 nanoseconds
	and not 7000 ns.
% subsection pipelining (end)

\subsection{Multiple Issue} % (fold)
\label{sub:multiple_issue}
	This type of processor replicates functional units
	and tries to simultaneously execute different instructions in a program.
	A typical example is to sum up values from two different arrays (x, y)
	and store them in a third (z).
	With a running variable (i),
	the follow additions can be done simultaneously by two different sets of functional units:\\
	\begin{center}
		z[i] = x[i] + y[i];
		\begin{enumerate}
			\item z[1] = x[1] + y[1]
			\item z[2] = x[2] + y[2]
		\end{enumerate}
	\end{center}
	\\
	There are two types of Multiple issue, static and dynamic.
	In Static Multiple Issue, the functional units are assigned
	and scheduled at the compile time of the code.
	In Dynamic Multiple Issue, the scheduling takes place at run-time.
	A processor supporting dynamic multiple issue is often called a ‘superscalar’.
% subsection multiple_issue (end)

\subsection{Speculation} % (fold)
\label{sub:speculation}
	In Speculative Execution the computers guesses which parts of the code can be executed in parallel.
	Not in all cases it is just as obvious as above,
	but in some cases it can be done.
	This depends on the code given to the computer.
% subsection speculation (end)

\subsection{Hardware Multithreading} % (fold)
\label{sub:hardware_multithreading}
	Threads offer a great opportunity to do introduce parallelism,
	as the programmer defines clearly what can be done in parallel.
	Supporting multiple threads on hardware level allows the system do useful work
	when the task being currently executed has stalled for any reason.
	Typical reasons are the wait for I/O or data to be loaded from memory.
	Multi-threading is characterized by the typical execution time for a thread.

	Fine-grained multithreading will switch threads in the processor after every instruction,
	thereby skipping threads that are currently stalled.
	This technique avoids wasting machine time,
	as it will skip every potential threat.
	A thread that consists of a long sequence of instruction will have to wait longer,
	as the instructions are only executed one by one.
	Simultaneous multithreading (SMT) is an even finer grained variant,
	that allows multiple threads to make use of multiple functional units.

	Coarse-grained multithreading only switches the threads if the current one stalls
	and has to wait for a time-consuming operation to complete.
	This leads to lesser thread switches,
	which allows threads with longer sequences of instructions to be executed in a row.
	It also is not that much dependent on fast switch between threads.
	Although the processor might idle for some time with this technique.
% subsection hardware_multithreading (end)
% section instruction_level_parallelism (end)
\section{Classification} % (fold)
\label{sec:classification}

\subsection{Single Instruction Multiple Dataset} % (fold)
\label{sub:single_instruction_multiple_dataset}
	This technique utilizes processor that are only capable of a few instructions,
	but can perform these very fast.
	They are used in parallel, executing the same instruction on the different data streams. 
	This was used extensively for vector processing for example.
	This architecture has some drawbacks,
	as all ALUs have to perform the same instruction or remain idle.
	In classic designs they were even required to operate synchronously
	and even todays ALUs do not have an instruction storage.
	Overall this makes SIMD efficient for large data parallel problems,
	but for nothing else.

	\subsubsection{Vector processors} % (fold)
	\label{ssub:vector_processors}
	In contrast to conventional CPUs who work on individual data elements of scalars
	vector processors operate on arrays or on vectors of data.
	They incorporate special register and are able to store a vector of operands 
	and perform an operation simultaneously on their contents.
	Therefore they are related to SIMD.

	Vector processors make use of interleaved memory organized in “banks”,
	that is accessible independently.
	The data they work on can be distributed over muliple banks,
	therefore allowing access by more functional units.
	This memory is also optimized to loading and storing of successive elements.

	This kind of processors is fast and easy to use. 
	Programmers can rely on compilers that are well optimized
	and can identify code that is vectorizable.
	Also they can provide information about code that is not vectorizable,
	which helps re evaluating it.
	Vector processors have a high memory bandwidth by design.
	On the other hand they are specialized on vectors,
	so they are not capable to process irregular data structures very well.
	Also they do not scale,
	as there is a finite limit to their abilities to handle larger problems.
	% subsubsection vector_processors (end)
	
	\subsubsection{Graphic Processing Units} % (fold)
	\label{ssub:graphic_processing_units}
	As these types of specialized processing units became faster,
	they could also be used for other things than graphical computations.
	They incorporate so called shader functions that are part of pipelines.
	These shader functions can be programmed with a few lines of C code.
	They are implicitly parallel, since that is what they were used for in the first place,
	to work on multiple elements of a graphics stream.
	GPUs use the SIMD parallelism although they are not pure SIMD devices.
	% subsubsection graphic_processing_units (end)
% subsection single_instruction_multiple_dataset (end)

% section classification (end)

\end{document}